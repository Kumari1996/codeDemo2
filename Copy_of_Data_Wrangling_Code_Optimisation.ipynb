{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "vHZpE9CeaAG-"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kumari1996/codeDemo2/blob/My-Branch/Copy_of_Data_Wrangling_Code_Optimisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **👨🏻‍🎓 Learning Objective 👨🏻‍🎓**"
      ],
      "metadata": {
        "id": "z6VJ8Phodljd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#first try in github- yesssssssssss"
      ],
      "metadata": {
        "id": "7PWFtTD2iLYw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Introduction**"
      ],
      "metadata": {
        "id": "6ZzWpxSnju6x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "👋 Hi students! In this lesson, we'll be discussing the importance of code optimization in data wrangling, techniques for optimizing code, best practices for efficient data wrangling, and strategies for parallel and distributed data wrangling.\n",
        "\n",
        "🐍 Data wrangling involves transforming and manipulating data to make it more useful for analysis. As datasets grow in size, data wrangling code can become slow and inefficient, leading to longer processing times and slower analysis.\n",
        "🤖 This is where code optimization comes in. Code optimization involves making your code more efficient and streamlined, to improve its performance and speed up processing times. This is especially important when working with large datasets.\n",
        "\n",
        "📊 In this lesson, we'll explore some techniques for optimizing your code, including using vectorized operations, avoiding loops, reducing memory usage, and parallel processing.\n",
        "\n",
        "🚀 We'll also discuss best practices for efficient data wrangling, such as understanding your data, keeping your code clean, and collaborating with others.\n",
        "\n",
        "💻 Finally, we'll discuss strategies for parallel and distributed data wrangling, which involve using multiple processors or distributed computing resources to speed up processing times and improve the performance of your code.\n",
        "\n",
        "📝 By understanding the importance of code optimization and using these techniques and best practices, you can improve the performance of your data wrangling code and make it easier to work with large datasets and perform complex analyses.\n",
        "\n",
        "🐍 So, let's dive in and explore the world of code optimization in data wrangling!\n"
      ],
      "metadata": {
        "id": "TZx87ZfD3LLm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Primary Goals**"
      ],
      "metadata": {
        "id": "C9DrFy85lY3T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "👋 Hi there! In this lesson, we'll be discussing the primary goals of understanding the importance of code optimization in data wrangling, techniques for code optimization, best practices for efficient data wrangling, and strategies for parallel and distributed data wrangling, using some wonderful emojis. 📊\n",
        "\n",
        "🐍 Here are the primary goals of this lesson:\n",
        "\n",
        "🤔 To understand the importance of code optimization in data wrangling and how it can improve the performance of your code and speed up processing times.\n",
        "\n",
        "🚀 To learn techniques for optimizing your code, such as using vectorized\n",
        "operations, avoiding loops, reducing memory usage, and parallel processing.\n",
        "\n",
        "📚 To explore best practices for efficient data wrangling, such as understanding your data, keeping your code clean, and collaborating with others.\n",
        "\n",
        "🤖 To understand strategies for parallel and distributed data wrangling, which involve using multiple processors or distributed computing resources to speed up processing times and improve the performance of your code.\n",
        "\n",
        "📝 By achieving these primary goals, you'll have a better understanding of how to optimize your data wrangling code and make it more efficient, allowing you to work with large datasets and perform complex analyses more easily.\n",
        "\n",
        "🐍 So, let's get started and learn how to optimize our data wrangling code using these wonderful techniques and best practices!\n"
      ],
      "metadata": {
        "id": "UJfDjA-w4EEK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **📖 Learning Material 📖**"
      ],
      "metadata": {
        "id": "PkyqSsKBZ1mm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Introduction**"
      ],
      "metadata": {
        "id": "4hCC1mG_Z4tI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "👋 You know that data wrangling, also known as data cleaning or data preparation, is the process of cleaning and transforming raw data into a more usable format. Code optimization, on the other hand, is the process of improving the performance of code by reducing its resource usage, improving its efficiency, and making it more maintainable.\n",
        "\n",
        "\n",
        "🔍 Data wrangling and code optimization are closely related because they both involve working with data and code to make it more usable and efficient. In the context of data wrangling, code optimization is important because the process of cleaning and transforming data can be computationally intensive and time-consuming. By optimizing the code, we can reduce the time and resources required to perform these tasks and make the data wrangling process more efficient.\n",
        "\n",
        "\n",
        "💻 There are several techniques that can be used to optimize code for data wrangling, such as vectorization, caching, and parallelization.\n",
        "\n",
        "Vectorization involves performing operations on entire arrays or matrices of data rather than on individual elements, which can significantly improve the performance of code.\n",
        "\n",
        "Caching involves storing the results of computationally intensive operations in memory so that they can be quickly accessed later, which can also improve performance.\n",
        "\n",
        "Parallelization involves splitting up a task into smaller, independent parts that can be executed simultaneously on multiple processors or cores, which can further improve performance."
      ],
      "metadata": {
        "id": "ncHsWBhlaHnD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Importance of Code Optimization in Data Wrangling**"
      ],
      "metadata": {
        "id": "giXBlRoeapdQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔍 Data wrangling involves cleaning, transforming, and preparing raw data for analysis, which can be computationally intensive and time-consuming.\n",
        "\n",
        "💻 Code optimization is the process of improving the performance of code by reducing its resource usage, improving its efficiency, and making it more maintainable.\n",
        "\n",
        "🌟 By optimizing their code for data wrangling, developers can reduce the time and resources required to perform tasks such as cleaning, transforming, and merging data.\n",
        "\n",
        "📈 Code optimization can also improve the scalability of data wrangling tasks, allowing data analysts and data scientists to work with larger datasets more efficiently.\n",
        "\n",
        "🚀 Some techniques for code optimization in data wrangling include:\n",
        "\n",
        "1. Vectorization, which involves performing operations on entire arrays or matrices of data rather than on individual elements\n",
        "\n",
        "2. Caching, which involves storing the results of computationally intensive operations in memory for faster access\n",
        "\n",
        "3. Parallelization, which involves splitting up a task into smaller, independent parts that can be executed simultaneously on multiple processors or cores\n",
        "\n",
        "4. Concurrency, which involves the ability to execute multiple tasks or processes simultaneously, either on the same processor/core (using techniques such as threads or asyncio) or across multiple processors/cores (using techniques such as multiprocessing or distributed computing).\n"
      ],
      "metadata": {
        "id": "FAvJVgYmbSQ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1. NumPy for vectorization:**\n",
        "\n"
      ],
      "metadata": {
        "id": "tmjHs3BBbsfs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NumPy is a popular Python library for numerical computing that allows users to perform operations on entire arrays or matrices of data rather than on individual elements. This is known as vectorization and can significantly improve the performance of code by reducing the number of loop iterations required. For example, consider the following code for calculating the dot product of two arrays:"
      ],
      "metadata": {
        "id": "xSXyg0zyAubl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "a = np.array([1, 2, 3])\n",
        "b = np.array([4, 5, 6])\n",
        "\n",
        "dot_product = 0\n",
        "for i in range(len(a)):\n",
        "    dot_product += a[i] * b[i]\n",
        "\n",
        "print(dot_product)"
      ],
      "metadata": {
        "id": "C3g3MAmfBlsm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a3f3a96-9cc8-44b7-d153-034d437b2c94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code uses a for loop to iterate over each element of the arrays and calculate the dot product. However, this can be slow and inefficient for large arrays. With NumPy, we can instead use the dot function to perform the same operation in a single step:"
      ],
      "metadata": {
        "id": "wMDBytxlb3HI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dot_product=np.dot(a,b)\n",
        "dot_product"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AoojRsf8Imo",
        "outputId": "101928ed-699d-4ef5-a15c-939b90de0f7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Activity 1:**\n"
      ],
      "metadata": {
        "id": "3gHnc_RaIJOr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can you use NumPy to find the sum of squares of the first 10 positive integers without using any loops?"
      ],
      "metadata": {
        "id": "pkpW9H66JjAV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**🤔 Hint:**\n",
        "\n",
        "*The sum of squares of the first n positive integers can be computed using the formula n(n+1)(2n+1)/6.*"
      ],
      "metadata": {
        "id": "359OcOcjJkKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "#Your Code Here\n",
        "\n",
        "#solution using loop\n",
        "def sum_square_loop(n):\n",
        "  sum = 0\n",
        "  for i in range(1,n+1):\n",
        "    sum = sum + (i**2)\n",
        "  return sum\n",
        "sum_square_loop(100)"
      ],
      "metadata": {
        "id": "SSpIxSf9bXz4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfe30227-e915-4c7b-a0bf-dc52466fbcfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 89 µs, sys: 0 ns, total: 89 µs\n",
            "Wall time: 95.1 µs\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "338350"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "#solution using loop\n",
        "def sum_square_without_loop(n):\n",
        "  return (n * (n+1) * (2*n+1))/6\n",
        "sum_square_without_loop(100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PLvZ8gl9sKo",
        "outputId": "1064737c-4f99-4aeb-d5a6-8d891b38b910"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 23 µs, sys: 2 µs, total: 25 µs\n",
            "Wall time: 30 µs\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "338350.0"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2. Joblib for Caching:**\n",
        "\n"
      ],
      "metadata": {
        "id": "1N2sZHmTb7qK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Caching involves storing the results of computationally intensive operations in memory for faster access. This can be especially useful in data wrangling tasks where the same operation may be performed multiple times on the same data. The joblib library provides a simple way to cache function calls in Python. For example, consider the following code for calculating the mean of a list of numbers:"
      ],
      "metadata": {
        "id": "rVYZ4_5qAyJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import Memory\n",
        "\n",
        "# create a Memory object to cache function calls\n",
        "mem = Memory(location='cache')\n",
        "\n",
        "# define a function to calculate the mean of a list of numbers\n",
        "@mem.cache\n",
        "def calc_mean(numbers):\n",
        "    print(\"Calculating mean...\")\n",
        "    total = sum(numbers)\n",
        "    return total / len(numbers)\n",
        "\n",
        "# define a list of numbers to use for testing\n",
        "my_list = [1, 2, 3, 4, 5]\n",
        "\n",
        "# call the function the first time, which will calculate the mean and cache the result\n",
        "result1 = calc_mean(my_list)\n",
        "\n",
        "# call the function again with the same argument, which will retrieve the cached result instead of recalculating it\n",
        "result2 = calc_mean([1, 2, 3, 4, 5, 6])\n",
        "\n",
        "# print the results\n",
        "print(result1)  # should be 3.0\n",
        "print(result2)  # should also be 3.0, and the \"Calculating mean...\" message should not be printed\n"
      ],
      "metadata": {
        "id": "2XiVqJRPB1x8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a71736c0-a329-4268-b1c8-6c1f37c0d4db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "________________________________________________________________________________\n",
            "[Memory] Calling __main__--content-<ipython-input-1ba9d3b9df3d>.calc_mean...\n",
            "calc_mean([1, 2, 3, 4, 5])\n",
            "Calculating mean...\n",
            "________________________________________________________calc_mean - 0.0s, 0.0min\n",
            "________________________________________________________________________________\n",
            "[Memory] Calling __main__--content-<ipython-input-1ba9d3b9df3d>.calc_mean...\n",
            "calc_mean([1, 2, 3, 4, 5, 6])\n",
            "Calculating mean...\n",
            "________________________________________________________calc_mean - 0.0s, 0.0min\n",
            "3.0\n",
            "3.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we start by importing the **Memory** class from the **joblib** library. We then create a Memory object with the **location** parameter set to **'cache'**, which specifies where to store the cached results.\n",
        "\n",
        "Next, we define a function called **calc_mean** that takes a list of numbers as input and calculates their mean. We decorate the function with the **@mem.cache** decorator, which tells joblib to cache the results of this function call based on its input arguments.\n",
        "\n",
        "We then define a list of numbers my_list to use for testing. We call the calc_mean function twice with the same argument, which should result in the function being executed only once and the second call retrieving the cached result.\n",
        "\n",
        "Finally, we print the results of the two function calls, which should both be 3.0 (the mean of the input list), and the \"Calculating mean...\" message should only be printed once, during the first function call.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0wJNCtcOAE7A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Activity 2**"
      ],
      "metadata": {
        "id": "XAVn6wENITm8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose you have a function that takes a long time to run and returns a dictionary. Can you use Joblib to cache the results of this function so that subsequent calls to the function with the same arguments return the cached value without running the function again?"
      ],
      "metadata": {
        "id": "01rpkNr1LsTb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**🤔 Hint:**\n",
        "\n",
        "*Use the Memory object from Joblib to create a memory cache for the function.*\n",
        "\n"
      ],
      "metadata": {
        "id": "yEYuYTrCLu4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Your Code Here"
      ],
      "metadata": {
        "id": "OSSW5QgEbd1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Solution\n",
        "from joblib import Memory\n",
        "\n",
        "# create a Memory object to cache function calls\n",
        "mem = Memory(location='cache')\n",
        "\n",
        "# define a function to calculate the mean of a list of numbers\n",
        "@mem.cache\n",
        "def student_results(dict):\n",
        "  print(\"Processing the Results...\")\n",
        "  result = {}\n",
        "  for i in dict.keys():\n",
        "    if dict[i] > 90:\n",
        "      result.update({i: 'Excellent'})\n",
        "    elif dict[i] > 80:\n",
        "      result.update({i: 'Very Good'})\n",
        "    else:\n",
        "      result.update({i: 'Pass'})\n",
        "  return result\n",
        "\n",
        "students = {'Alice': 85, 'Bob': 95, 'Charlie': 75}\n",
        "\n",
        "#Function Call 1\n",
        "print(student_results(students))\n",
        "\n",
        "#Function Call 2\n",
        "print(student_results(students))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xczN2sYTA5Qe",
        "outputId": "75a81672-02af-4f44-dfd1-8eb4157d0b88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "________________________________________________________________________________\n",
            "[Memory] Calling __main__--content-<ipython-input-d169f96c03d6>.student_results...\n",
            "student_results({'Alice': 85, 'Bob': 95, 'Charlie': 75})\n",
            "Processing the Results...\n",
            "__________________________________________________student_results - 0.0s, 0.0min\n",
            "{'Alice': 'Very Good', 'Bob': 'Excellent', 'Charlie': 'Pass'}\n",
            "{'Alice': 'Very Good', 'Bob': 'Excellent', 'Charlie': 'Pass'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##<b>3. Pandarallel for Parallelization"
      ],
      "metadata": {
        "id": "jgmXW0XDCPhP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's install Pandarallel module first."
      ],
      "metadata": {
        "id": "AoRewGFUCkfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandarallel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6SY8uDVBssg",
        "outputId": "1496092b-a280-4f94-89e4-869a292d797f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pandarallel\n",
            "  Downloading pandarallel-1.6.5.tar.gz (14 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting dill>=0.3.1 (from pandarallel)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1 in /usr/local/lib/python3.10/dist-packages (from pandarallel) (1.5.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from pandarallel) (5.9.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1->pandarallel) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1->pandarallel) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas>=1->pandarallel) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1->pandarallel) (1.16.0)\n",
            "Building wheels for collected packages: pandarallel\n",
            "  Building wheel for pandarallel (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pandarallel: filename=pandarallel-1.6.5-py3-none-any.whl size=16677 sha256=9b2f1a4c8ef6d17735e91c33a5d64797b9ee036b682a28ec2e198d0203498c14\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/4f/1e/34e057bb868842209f1623f195b74fd7eda229308a7352d47f\n",
            "Successfully built pandarallel\n",
            "Installing collected packages: dill, pandarallel\n",
            "Successfully installed dill-0.3.6 pandarallel-1.6.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pandarallel import pandarallel\n",
        "\n",
        "# initialize pandarallel\n",
        "pandarallel.initialize()\n",
        "\n",
        "# create a sample dataframe\n",
        "df = pd.DataFrame({'A': [1, 2, 3, 4, 5], 'B': [6, 7, 8, 9, 10]})\n",
        "\n",
        "# define a function to apply to each row of the dataframe\n",
        "def my_func(row):\n",
        "    return row['A'] + row['B']\n",
        "\n",
        "# use pandarallel to apply the function to each row in parallel\n",
        "result = df.parallel_apply(my_func, axis=1)\n",
        "\n",
        "# print the result\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fKDqhq4Bpnh",
        "outputId": "29a11a29-867d-4e1b-b381-d1a31f45a062"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Pandarallel will run on 1 workers.\n",
            "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
            "0     7\n",
            "1     9\n",
            "2    11\n",
            "3    13\n",
            "4    15\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##<b>Activity 3"
      ],
      "metadata": {
        "id": "v8IRStFkIb2v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can you use Pandarallel to parallelize a Pandas apply operation on a DataFrame in Google Colab?\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1. Create a Pandas DataFrame containing a list of integers.\n",
        "\n",
        "2. Define a custom function that squares a number.\n",
        "\n",
        "3. Use Pandas apply method to apply the custom function to each element of the DataFrame.\n",
        "\n",
        "4. Parallelize the apply operation using Pandarallel.\n",
        "\n",
        "5. Compare the execution times of the apply operation with and without Pandarallel.\n",
        "\n",
        "**Hints:**\n",
        "\n",
        "1. Use the \"pandarallel\" library to enable parallel processing.\n",
        "\n",
        "2. Use the \"apply\" method with the \"parallel=True\" option to parallelize the apply operation.\n"
      ],
      "metadata": {
        "id": "lDKT2Wn9R-W8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pandarallel import pandarallel\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "UcaRdFAPPadC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize pandarallel\n",
        "pandarallel.initialize()\n",
        "\n",
        "# create a sample dataframe\n",
        "df = pd.DataFrame({'Numbers': [np.arange(1,10)]})\n",
        "\n",
        "# define a function to apply to each row of the dataframe\n",
        "def my_func(num):\n",
        "    return num**2\n",
        "\n",
        "# use pandarallel to apply the function to each row in parallel\n",
        "result = df.parallel_apply(my_func)\n",
        "\n",
        "# print the result\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zw7vX8CfFV1m",
        "outputId": "50065680-bf9c-468e-fa30-db7fa3612e5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Pandarallel will run on 1 workers.\n",
            "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
            "                             Numbers\n",
            "0  [1, 4, 9, 16, 25, 36, 49, 64, 81]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a sample dataframe\n",
        "df = pd.DataFrame({'Numbers': [np.arange(1,10)]})\n",
        "\n",
        "# define a function to apply to each row of the dataframe\n",
        "def my_func(num):\n",
        "    return num**2\n",
        "\n",
        "# use pandarallel to apply the function to each row in parallel\n",
        "result = df.apply(my_func)\n",
        "\n",
        "# print the result\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFATnJ2WPBpQ",
        "outputId": "379d66d9-6f39-46e4-a162-6d728036aed7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                             Numbers\n",
            "0  [1, 4, 9, 16, 25, 36, 49, 64, 81]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##<b>4. Concurrency"
      ],
      "metadata": {
        "id": "ZryrTEGkEXiA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "C_GTYTUQJlmB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Concurrency is achieved by dividing the task into smaller parts that can be executed independently and concurrently. Concurrency can be achieved using multiple techniques such as threads, asyncio, and multiprocessing.\n",
        "\n",
        "Here is an example of how to achieve concurrency using the threading library:"
      ],
      "metadata": {
        "id": "VdVM4FoAEwu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "import time\n",
        "\n",
        "status = False\n",
        "\n",
        "def worker():\n",
        "  c = 0\n",
        "  while not status:\n",
        "    time.sleep(1)\n",
        "    c = c + 1\n",
        "    print(c)\n",
        "#worker()\n",
        "threading.Thread(target=worker).start()\n",
        "\n",
        "sta = input(\"Enter Exit to terminate : \")\n",
        "if sta == 'Exit':\n",
        "   status = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-WVEjegJv36",
        "outputId": "44f160ca-5d46-4eda-d812-e0f2ee2757ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "Enter Exit to terminate : Exit\n",
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This demonstrates how concurrency allows us to execute multiple tasks simultaneously, making our code more efficient and responsive."
      ],
      "metadata": {
        "id": "faK_7_n9Fd8z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##<b>🤔Do you Know🤔"
      ],
      "metadata": {
        "id": "1DXT-RsxEdMD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parallelization and concurrency are related concepts but they refer to different ways of achieving efficient use of computer resources.\n",
        "\n",
        "Concurrency refers to the ability to execute multiple tasks or processes simultaneously. This can be achieved through techniques such as threads, asyncio, and multiprocessing. With concurrency, multiple tasks can be executed at the same time, but each task is executed on the same processor or core. Concurrency allows you to execute tasks in an overlapping manner, which can improve responsiveness and throughput.\n",
        "\n",
        "Parallelization, on the other hand, refers to the ability to execute multiple tasks or processes simultaneously across multiple processors or cores. Parallelization can be achieved through techniques such as multiprocessing, multithreading, or distributed computing. With parallelization, multiple tasks can be executed on different processors or cores simultaneously, allowing you to achieve higher throughput and better performance.\n",
        "\n",
        "In summary, concurrency is about executing multiple tasks simultaneously on the same processor or core, while parallelization is about executing multiple tasks simultaneously on multiple processors or cores. Both techniques can be used to achieve efficient use of computer resources, but they have different advantages and limitations depending on the task at hand."
      ],
      "metadata": {
        "id": "pnKcA88lEh4w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Best Practices for Efficient Data Wrangling**"
      ],
      "metadata": {
        "id": "DqI5oDOUdjX_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are some best practices for Efficient Data Wrangling:"
      ],
      "metadata": {
        "id": "sGn1AMIPeLpU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**🔍 1. Understand the Data:**\n",
        "\n",
        "Before starting any data wrangling task, it is important to thoroughly understand the data and its structure. This includes knowing the data types, column names, missing values, and any potential outliers. This can help in identifying the appropriate data cleaning and transformation techniques needed.\n",
        "\n",
        "**📝 2. Document the Process:**\n",
        "\n",
        "Keeping track of the steps taken during data wrangling can be helpful for future reference and troubleshooting. This includes documenting the data sources, cleaning and transformation techniques used, and any assumptions made during the process.\n",
        "\n",
        "**🚀 3. Use Vectorization:**\n",
        "\n",
        "Vectorization is a technique that allows for performing operations on an entire array or data frame at once, instead of looping through each element. This can significantly improve the performance of data wrangling tasks, especially when dealing with large datasets. Libraries such as NumPy and Pandas provide vectorization capabilities.\n",
        "\n",
        "**💻 4. Utilize Parallelization:**\n",
        "\n",
        "Parallelization is another technique that can improve the efficiency of data wrangling tasks by breaking down the workload into smaller tasks that can be performed simultaneously. Libraries such as Dask and joblib provide parallelization capabilities for data wrangling tasks.\n",
        "\n",
        "**🧹 5. Handle Missing Data:**\n",
        "\n",
        "Missing data can cause issues during data wrangling and analysis. It is important to handle missing data appropriately, either by imputing values or removing observations. Pandas provides methods for handling missing data, such as fillna() and dropna().\n",
        "\n",
        "**🤖 6. Automate Where Possible:**\n",
        "\n",
        "Automation can help to streamline repetitive data wrangling tasks and reduce the potential for errors. Tools such as Python scripts and workflows in tools like Apache Airflow can help automate data wrangling processes.\n",
        "\n",
        "**📊 7. Test and Validate:**\n",
        "\n",
        "Testing and validating the data wrangling process is important to ensure accuracy and reliability of the final output. This can include checking for data consistency, confirming data types, and validating the final output against expectations.\n",
        "\n",
        "\n",
        "Example of using vectorization in Pandas:"
      ],
      "metadata": {
        "id": "OrRJCUXtaogm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a sample data frame\n",
        "df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n",
        "\n",
        "# Multiply each element in column A by 2 using vectorization\n",
        "df['A'] = df['A'] * 2\n",
        "\n",
        "# Print the updated data frame\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDxMonDmChmj",
        "outputId": "aa822a06-e916-47b8-bd88-5321abc5a7ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   A  B\n",
            "0  2  4\n",
            "1  4  5\n",
            "2  6  6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **✅ Summary ✅**"
      ],
      "metadata": {
        "id": "mNiFWmlVZ88w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "👋 Hi students! Now that we've covered the importance of code optimization in data wrangling, techniques for optimization, best practices for efficient data wrangling, and strategies for parallel and distributed data wrangling using some wonderful emojis, let's review what we've learned. 📊\n",
        "\n",
        "🤖 Code optimization is important in data wrangling as it can improve the performance of your code and speed up processing times.\n",
        "\n",
        "🐍 Techniques for code optimization include using vectorized operations, avoiding loops, reducing memory usage, and parallel processing.\n",
        "\n",
        "📚 Best practices for efficient data wrangling include understanding your data, keeping your code clean, and collaborating with others.\n",
        "\n",
        "💻 Strategies for parallel and distributed data wrangling involve using multiple processors or distributed computing resources to speed up processing times and improve the performance of your code.\n",
        "\n",
        "📈 By using these techniques and best practices, you can optimize your data wrangling code and make it more efficient, allowing you to work with large datasets and perform complex analyses more easily.\n",
        "\n",
        "📝 It's important to keep in mind that code optimization is an ongoing process and requires continuous effort and improvement. By constantly seeking ways to improve your code and staying up-to-date with the latest techniques and tools, you can become an expert in data wrangling and analysis.\n",
        "\n",
        "🐍 Keep practicing and applying these techniques and best practices, and you'll be well on your way to mastering data wrangling in no time!\n"
      ],
      "metadata": {
        "id": "tEOWbMIlZ_kH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **➕ Additional Reading ➕**"
      ],
      "metadata": {
        "id": "vHZpE9CeaAG-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Mnemonic**\n",
        "\n",
        "👋 Hi there! Let me tell you the story of CoderWorlds, a data analytics company that used the concepts of understanding the importance of code optimization in data wrangling, techniques for code optimization, best practices for efficient data wrangling, and strategies for parallel and distributed data wrangling using some wonderful emojis.\n",
        "\n",
        "📈 CoderWorlds specializes in data analytics and works with a wide range of clients from various industries. They understand the importance of code optimization in data wrangling and continuously seek ways to improve the performance of their code.\n",
        "\n",
        "🐍 To achieve this, CoderWorlds uses techniques such as vectorized operations, avoiding loops, reducing memory usage, and parallel processing to optimize their code and speed up processing times.\n",
        "\n",
        "📚 They also follow best practices for efficient data wrangling, such as understanding their data, keeping their code clean, and collaborating with other team members.\n",
        "\n",
        "💻 In addition, CoderWorlds uses strategies for parallel and distributed data wrangling, such as using multiple processors or distributed computing resources, to further speed up processing times and improve the performance of their code.\n",
        "\n",
        "🚀 Through their commitment to code optimization and efficient data wrangling practices, CoderWorlds has been able to provide their clients with high-quality data analysis services, enabling them to make better business decisions based on accurate and reliable data.\n",
        "\n",
        "📝 By following in the footsteps of CoderWorlds and utilizing these concepts, you too can optimize your data wrangling code and improve the performance of your data analysis, making it easier to work with large datasets and perform complex analyses.\n",
        "\n",
        "🐍 Keep practicing and applying these techniques and best practices, and you'll be well on your way to becoming an expert in data wrangling and analysis, just like CoderWorlds!\n"
      ],
      "metadata": {
        "id": "iKCmcguUaCvp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Best Practices/Tips**\n",
        "🤔 Understand your data: Before you start working with your data, it's important to understand its structure, format, and any potential issues that may arise. This can help you optimize your code more effectively and avoid potential errors.\n",
        "\n",
        "🐍 Use vectorized operations: Vectorized operations can perform multiple calculations simultaneously, making them much faster than looping over individual elements. Use them whenever possible to optimize your code and improve performance.\n",
        "\n",
        "🧹 Keep your code clean: Writing clean, well-organized code can improve the readability of your code and make it easier to optimize. Use consistent naming conventions, comments, and indentation to keep your code organized and understandable.\n",
        "\n",
        "📏 Reduce memory usage: Large datasets can quickly use up memory, leading to slow performance or even crashes. Use techniques such as data compression or chunking to reduce the amount of memory used by your code and improve its performance.\n",
        "\n",
        "🤖 Use parallel processing: By using multiple cores or processors to run your code, you can speed up processing times and improve the performance of your code. Consider using libraries such as Dask or PySpark to enable parallel processing.\n",
        "\n",
        "📚 Collaborate with others: Collaboration can help you identify potential issues with your code and find new ways to optimize it. Share your code with other team members and seek feedback to improve its performance.\n"
      ],
      "metadata": {
        "id": "wV31TCvh92rH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###<b>Shortcomings</b>\n",
        "\n",
        "\n",
        "🕰️ Time-consuming: Code optimization can be time-consuming, especially when working with large datasets. It can require significant effort and trial-and-error to find the most efficient code.\n",
        "\n",
        "🤔 Requires expertise: Code optimization requires a deep understanding of programming concepts and techniques. It may require additional training or education to become proficient in these skills.\n",
        "\n",
        "📈 Diminishing returns: There may be a point where further optimization doesn't significantly improve performance. It's important to balance optimization efforts with the amount of time and resources required.\n",
        "\n",
        "🐌 Limited hardware: Some techniques for optimization, such as parallel processing, require specialized hardware. If you don't have access to this hardware, you may be limited in your optimization efforts.\n",
        "\n",
        "💻 Compatibility issues: Some optimization techniques may not be compatible with all systems or libraries. It's important to test your code on multiple systems to ensure compatibility.\n"
      ],
      "metadata": {
        "id": "lEJlHV202-ez"
      }
    }
  ]
}